---
name: Sotaro Kimura
title: "Start playing with Spark Structured Streaming locally and build up a stream processing pipeline"
length: 40
audience: Beginner
language: Japanese
twitter: kimutansk
github: kimutansk
icon: https://s.gravatar.com/avatar/5d869b93035410ece41ff0d86d8c5c14
organization: DWANGO Co., Ltd.
tags:
  - Big Data / Fast Data
  - Tools
suggestions:
  - People who are interested in Spark and want to learn basics
  - People who want to know practical use cases of Spark
  - People who want to build a stream processing system quickly
---
Spark is a framework for parallel stream processing, however, that probably sounds like something difficult to work with even if you are interested.
In reality, Spark is a data processing tool which allows you execute it on your local machine as well as a large-scale cluster.
At the same time, Scala users will feel familiar with it as it is OSS in Scala.
Thus, you can start it quickly with your local machine to run a data processing pipeline in Scala and then keep building upon it
Moreover, processing structured data types like string and numeral types, as well as stream processing are both possible.
This session will introduce how to build up a stream processing pipeline easily using Spark's new feature, Spark Structured Streaming,
which allows you to concisely write stream data processing as structured data.
